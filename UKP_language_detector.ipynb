{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UKP_language_detector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQcQ8DBAZwILrixTpvpKk1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iarroyof/ukp_app/blob/main/UKP_language_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzrI7OypZ81a"
      },
      "source": [
        "from collections import deque\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        " \n",
        "def tokenize(doc, ngram_range=(1, 3)):\n",
        " \n",
        "    collected_ngrams = []\n",
        "    for ng_size in range(*ngram_range):\n",
        "        window = deque(maxlen=ng_size)\n",
        "        for ch in doc:\n",
        "            window.append(ch)\n",
        "            collected_ngrams.append(''.join(list(window)))\n",
        " \n",
        "    return collected_ngrams"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05e8c6TnYM-E"
      },
      "source": [
        "def accuracy(y, y_hat):\n",
        "    eqs = 0\n",
        "    for a, b in zip(y, y_hat):\n",
        "        if a == b:\n",
        "            eqs += 1\n",
        " \n",
        "    return eqs / len(y)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crwfyuxjY56W"
      },
      "source": [
        " \n",
        " \n",
        "def prepare_data(file_url, sample=0.01):\n",
        "# Load dataset\n",
        " \n",
        "    dataset = pd.read_csv(file_url).sample(frac=sample)\n",
        "    train_data = dataset.iloc[0:int(0.7 * len(dataset.index))]\n",
        "    test_data = dataset.iloc[int(0.7 * len(dataset.index)):]\n",
        " \n",
        "    X_train_data = train_data.Text.apply(tokenize)\n",
        "    X_train_data = X_train_data.apply(set)\n",
        "    X_train_data = X_train_data.apply(list)\n",
        "    Y_train_data = train_data.Language\n",
        " \n",
        "    Xtrain = []\n",
        "    Ytrain = []\n",
        " \n",
        "    for x, y in zip(X_train_data, Y_train_data):\n",
        "        Ytrain += [y] * len(x)\n",
        "        Xtrain += x\n",
        " \n",
        "    X_test_data = test_data.Text.apply(tokenize)\n",
        "    X_test_data = X_test_data.apply(set)\n",
        "    X_test_data = X_test_data.apply(list)\n",
        "    Y_test_data = test_data.Language\n",
        " \n",
        "    Ytest = []\n",
        "    Xtest = []\n",
        "    for x, y in zip(X_test_data, Y_test_data):\n",
        "        Ytest += [y] * len(x)\n",
        "        Xtest += x\n",
        " \n",
        "    return Xtrain, Ytrain, Xtest, Ytest"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5C1_4qBn1fi"
      },
      "source": [
        " \n",
        "class BayesClassifier:\n",
        " \n",
        "    def __init__(self, ngram_range=(1, 3), out_distributions=False):\n",
        "        self.ngram_range = ngram_range\n",
        "        self.out_dist = out_distributions\n",
        " \n",
        " \n",
        "    def tokenize(self, doc):\n",
        " \n",
        "        collected_ngrams = []\n",
        "        for ng_size in range(*self.ngram_range):\n",
        "            window = deque(maxlen=ng_size)\n",
        "            for ch in doc:\n",
        "                window.append(ch)\n",
        "                collected_ngrams.append(''.join(list(window)))\n",
        " \n",
        "        return collected_ngrams\n",
        " \n",
        " \n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"Bayes training\n",
        "        I first create a contingecy table\n",
        "        Each cell contains the result of the indicator product function \n",
        "        f(x, y) = 1 if x == x' and y == y' ? 0 otherwise.\n",
        " \n",
        "        I create sample spaces for each RV\"\"\" \n",
        "        (self.omega_x, Tx) = np.unique(X, return_counts=True)\n",
        "        (self.omega_y, Ty) = np.unique(Y, return_counts=True)\n",
        " \n",
        "        # Create contigency table (Kronecker product)\n",
        "        f_xy = {}\n",
        "        for x in self.omega_x:\n",
        "            for y in self.omega_y:\n",
        "                f_xy[(x, y)] = sum([int(x_ == x and y_ == y)\n",
        "                    for x_, y_ in zip(X, Y)])\n",
        " \n",
        " \n",
        "         # Posterior computations\n",
        "        self.PYgX = {}\n",
        "        for y in self.omega_y:\n",
        "            for x in self.omega_x:\n",
        "                Zx = sum([f_xy[(x, y_)] for y_ in self.omega_y])\n",
        "                self.PYgX[(y, x)] = f_xy[(x, y)] / Zx\n",
        " \n",
        "        return self\n",
        " \n",
        " \n",
        "    def posterior(self, text):\n",
        " \n",
        "        tokens = list(set(self.tokenize(text)))\n",
        "        pmfs = []\n",
        "        for x in tokens:\n",
        "            try:\n",
        "                pmfs.append([self.PYgX[(y, x)] for y in self.omega_y])\n",
        "            except KeyError:\n",
        "                pass\n",
        " \n",
        "        prod = [1.0] * len(self.omega_y) \n",
        "        for ygx in pmfs:\n",
        "            prod = np.multiply(prod, ygx)\n",
        "        if self.out_dist:\n",
        "            return list(zip(self.omega_y, prod))\n",
        "        else:\n",
        "            return self.omega_y[np.argmax(prod)]\n",
        " \n",
        " \n",
        "    def predict(self, X):\n",
        "        predictions = [self.posterior(x) for x in X]\n",
        "        return predictions"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo_DOpshwVj7"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/iarroyof/ukp_app/main/Language%20Detection.csv\"\n",
        " \n",
        "X_train, Y_train, X_test, Y_test = prepare_data(url, sample=0.01)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4O3O358LYzi",
        "outputId": "85481aa1-c0ee-4b6f-dd69-9120a8a2bfe8"
      },
      "source": [
        " \n",
        "bayes = BayesClassifier(ngram_range=(2, 3))\n",
        " \n",
        "bayes.fit(X_train, Y_train)\n",
        " \n",
        "Y_hat = bayes.predict(X_test)\n",
        " \n",
        "accuracy(Y_test, Y_hat)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4206807964033398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    }
  ]
}